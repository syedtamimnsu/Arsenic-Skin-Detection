# -*- coding: utf-8 -*-
"""Arsenic Skin Image Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GpBKScHq9BOGeJ7L3tth4kH_R3nOxQhZ
"""

!pip install -q gdown

!gdown --id 1TT-Z1Y9ZLUheDwla31PvLlKeLDT8rJqT --output ArsenicSkinImageBD.zip

!ls

#extracting the compessed dataset
from zipfile import ZipFile
dataset = '/content/ArsenicSkinImageBD.zip'

with ZipFile(dataset, 'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

!ls

import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mping
import cv2
from google.colab.patches import cv2_imshow
from PIL import Image
from sklearn.model_selection import train_test_split

infected_files = os.listdir('/content/ArsenicSkinImageBD/Augmented/infected')
print(infected_files[0:5])
print(infected_files[-5:])

not_infected_files = os.listdir('/content/ArsenicSkinImageBD/Augmented/not_infected')
print(not_infected_files[0:5])
print(not_infected_files[-5:])

print('Number of with infected images: ',len(infected_files))
print('Number of without infected images: ',len(not_infected_files))

"""Create Labels for the two classes

With infected --> 1

Without infected --> 0
"""

#create the labels

infected_labels = [1]*len(infected_files)
not_infected_labels = [0]*len(not_infected_files)

print(len(infected_labels))

print(len(not_infected_labels))

print(infected_labels[0:5])

print(not_infected_labels[0:5])

#combine with and without infected lebels in one list

labels = infected_labels + not_infected_labels

print(len(labels))
print(labels[0:5])
print(labels[-5:])

#display with infected image
img = mping.imread('/content/ArsenicSkinImageBD/Augmented/infected/IMG_1187_augmented_6.png')
implot = plt.imshow(img)
plt.show()

#display without infected image
img = mping.imread('/content/ArsenicSkinImageBD/Augmented/not_infected/IMG_3352_augmented_5.png')
implot = plt.imshow(img)
plt.show()

"""Image Pricesssing

Resize images

Convert the images into numpy arrrays
"""

#convert image into numpay array

#with infected images
infected_path = '/content/ArsenicSkinImageBD/Augmented/infected/'

data = []

for img_file in infected_files:

  image = Image.open(infected_path + img_file)
  #resize image as 128*128
  image = image.resize((128,128))
  #convert image into RGB color
  image = image.convert('RGB')
  #cinvert into numpy array
  image = np.array(image)
  data.append(image)



#without infected mask images

not_infected_path = '/content/ArsenicSkinImageBD/Augmented/not_infected/'

for img_file in not_infected_files:

  image = Image.open(not_infected_path + img_file)
  #resize image as 128*128
  image = image.resize((128,128))
  #convert image into RGB color
  image = image.convert('RGB')
  #cinvert into numpy array
  image = np.array(image)
  data.append(image)

type(data)

type(data[0])

len(data)

data[6]

data[0].shape

#converting image list and label into numpy array

x = np.array(data)
y = np.array(labels)

type(x)

type(y)

print(x.shape)
print(y.shape )

"""Train Test Split"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)

#scalling data

x_train_scaled = x_train / 255

x_test_scaled = x_test / 255

x_train_scaled[0]

"""Building Convolutional Neural Network (CNN)"""

import tensorflow as tf
from tensorflow import keras

num_of_classes = 2

model = keras.Sequential()

#first Convolutional layer
model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(128,128,3)))
model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))


#second Convolutional layer
model.add(keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))
model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))

#flatten Layer. its conver as vector
model.add(keras.layers.Flatten())

#first dense layer
model.add(keras.layers.Dense(128, activation='relu'))
model.add(keras.layers.Dropout(0.5))

#second dense layer
model.add(keras.layers.Dense(64, activation='relu'))
model.add(keras.layers.Dropout(0.5))

#third dense layer
#model.add(keras.layers.Dense(32, activation='relu'))
#model.add(keras.layers.Dropout(0.5))

#output layer
model.add(keras.layers.Dense(num_of_classes, activation='softmax'))

#compile the neural network

# model.compile(
#     optimizer = 'adam',
#     loss = 'sparse_categorical_crossentropy',
#     metrics = ['acc']
# )

#training the neural network

# history = model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=10)

"""Model Evaluation

"""

#test accuracy

# loss, accuracy = model.evaluate(x_test_scaled, y_test)
# print('Test Accuracy = ', accuracy)

# h = history

# #plot the loss value
# plt.plot(h.history['loss'], label='train loss')
# plt.plot(h.history['val_loss'], label='validation loss')
# plt.legend()
# plt.show()


# #plot the accuracy value
# plt.plot(h.history['acc'], label='train accuracy')
# plt.plot(h.history['val_acc'], label='validation accuracy')
# plt.legend()
# plt.show()

"""Building ResNet50"""

from tensorflow.keras import Sequential, models, layers
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras import optimizers

convolutional_base = ResNet50(weights='imagenet', include_top=False, input_shape=(256,256,3))

#setting up ResNet50

number_of_classes = 2

model = models.Sequential()
#convert(32,32,3) to (256,256,3)
#model.add(layers.UpSampling2D((2,2)))    #its convert (64,64,3)
#model.add(layers.UpSampling2D((2,2)))    #its convert (128,128,3)
model.add(layers.UpSampling2D((2,2)))    #its convert (256,256,3)
model.add(convolutional_base)
model.add(layers.Flatten())
#hiddel layer 1
model.add(layers.BatchNormalization())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
#hidden layer 2
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
#input layer
model.add(layers.BatchNormalization())
model.add(layers.Dense(number_of_classes, activation='softmax'))

model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['acc'])

history = model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=10)

#test accuracy
loss, accuracy = model.evaluate(x_test_scaled, y_test)
print('Test Accuracy = ', accuracy)

h = history

#plot the loss value
plt.plot(h.history['loss'], label='train loss')
plt.plot(h.history['val_loss'], label='validation loss')
plt.legend()
plt.show()


#plot the accuracy value
plt.plot(h.history['acc'], label='train accuracy')
plt.plot(h.history['val_acc'], label='validation accuracy')
plt.legend()
plt.show()

import pickle

# ... (your existing code) ...

# After training the model (after model.fit)
with open('Arsenic_Skin_Detection.pkl', 'wb') as file:
  pickle.dump(model, file)

print("Model saved as Face_Mask_Detection_model.pkl")

